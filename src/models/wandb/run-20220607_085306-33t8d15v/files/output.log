
Epoch 0:   0%|▏                                                        | 4/1121 [00:00<00:35, 31.27it/s, loss=1.95, v_num=d15v]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
  | Name  | Type        | Params
--------------------------------------
0 | model | SimpleModel | 769 K
--------------------------------------
769 K     Trainable params
0         Non-trainable params
769 K     Total params
3.080     Total estimated model params size (MB)
/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:495: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
                not been set for this class (_ResultMetric). The property determines if `update` by
                default needs access to the full metric state. If this is not the case, significant speedups can be
                achieved and we recommend setting this to `False`.
                We provide an checking function
                `from torchmetrics.utilities import check_forward_no_full_state`
                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
                default for now) or if `full_state_update=False` can be used safely.
  warnings.warn(*args, **kwargs)
/home/xps/anaconda3/envs/deeplearning/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.














Epoch 0:  84%|██████████████████████████████████████████████▎        | 944/1121 [00:30<00:05, 30.98it/s, loss=1.62, v_num=d15v]















Epoch 1:  81%|████████████████████████████████████████████▎          | 903/1121 [00:28<00:06, 31.63it/s, loss=1.57, v_num=d15v]

















Epoch 2:  88%|████████████████████████████████████████████████▎      | 984/1121 [00:31<00:04, 31.44it/s, loss=1.46, v_num=d15v]














Epoch 3:  90%|████████████████████████████████████████████████▋     | 1010/1121 [00:26<00:02, 38.83it/s, loss=1.44, v_num=d15v]












Epoch 4:  91%|████████████████████████████████████████████████▉     | 1016/1121 [00:24<00:02, 42.03it/s, loss=1.39, v_num=d15v]













Epoch 5:  84%|██████████████████████████████████████████████▎        | 943/1121 [00:26<00:04, 35.89it/s, loss=1.36, v_num=d15v]














Epoch 6:  88%|████████████████████████████████████████████████▌      | 990/1121 [00:26<00:03, 36.86it/s, loss=1.32, v_num=d15v]












Epoch 7:  80%|████████████████████████████████████████████           | 899/1121 [00:22<00:05, 39.23it/s, loss=1.33, v_num=d15v]















Epoch 8:  82%|████████████████████████████████████████████▉          | 916/1121 [00:28<00:06, 32.02it/s, loss=1.26, v_num=d15v]




