/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['model'])`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Sanity Checking DataLoader 0: 100%|██████████████████████████████████████████████| 2/2 [00:00<00:00,  6.92it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name  | Type | Params
-------------------------------
0 | model | VGG  | 139 M
-------------------------------
139 M     Trainable params
0         Non-trainable params
139 M     Total params
558.440   Total estimated model params size (MB)
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:487: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.
  rank_zero_warn(
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('dim:', ...)` in your `validation_step` but the value needs to be floating point. Converting it to torch.float32.
  warning_cache.warn(
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.





















Epoch 0:  92%|████████████████████████████████████▊   | 852/927 [00:43<00:03, 19.67it/s, loss=1.47, v_num=o84e]





















Epoch 1:  80%|████████████████████████████████        | 744/927 [00:41<00:10, 17.95it/s, loss=1.25, v_num=o84e]























Epoch 2:  86%|██████████████████████████████████▎     | 796/927 [00:42<00:06, 18.74it/s, loss=1.09, v_num=o84e]























Epoch 3:  89%|██████████████████████████████████▌    | 822/927 [00:42<00:05, 19.22it/s, loss=0.871, v_num=o84e]






















Epoch 4:  80%|████████████████████████████████        | 742/927 [00:41<00:10, 17.92it/s, loss=0.75, v_num=o84e]























Epoch 5:  87%|█████████████████████████████████▋     | 802/927 [00:42<00:06, 18.90it/s, loss=0.657, v_num=o84e]





Epoch 6:   9%|███▍                                    | 80/927 [00:04<00:46, 18.12it/s, loss=0.408, v_num=o84e]
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...

Testing DataLoader 0:   4%|█▋                                              | 253/7178 [00:00<00:17, 392.08it/s]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/media/data/chitb/.local/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:229: UserWarning: You called `self.log('test accuracy: ', ...)` in your `test_step` but the value needs to be floating point. Converting it to torch.float32.









Testing DataLoader 0: 100%|███████████████████████████████████████████████| 7178/7178 [00:19<00:00, 359.63it/s]
───────────────────────────────────────────────────────────────────────────────────────────────────────────────
       Test metric             DataLoader 0
───────────────────────────────────────────────────────────────────────────────────────────────────────────────
     test accuracy:         0.5660350918769836
